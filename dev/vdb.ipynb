{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c0e93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='lending_club_loans')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from qdrant_client.http import models\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from typing import List\n",
    "import time\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://395ca2ca-de28-4191-9469-82c422d8d06a.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.uRzKY7J7BGhAArzf_RqZlFht6HR4rRggFLbsyNGP89Y\",\n",
    "    timeout=360,\n",
    "    prefer_grpc=True\n",
    ")\n",
    "\n",
    "print(qdrant_client.get_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f85ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loan_text(row) -> str:\n",
    "    \"\"\"Create a text representation of each loan for embedding\"\"\"\n",
    "    text_parts = []\n",
    "\n",
    "    if pd.notna(row.get(\"Loan Title\")):\n",
    "        text_parts.append(f\"Loan Title: {row['Loan Title']}\")\n",
    "    if pd.notna(row.get(\"purpose\")):\n",
    "        text_parts.append(f\"Purpose: {row['purpose']}\")\n",
    "    if pd.notna(row.get(\"emp_title\")):\n",
    "        text_parts.append(f\"Employment: {row['emp_title']}\")\n",
    "    if pd.notna(row.get(\"Employment Length\")):\n",
    "        text_parts.append(f\"Employment Length: {row['Employment Length']}\")\n",
    "    if pd.notna(row.get(\"home_ownership\")):\n",
    "        text_parts.append(f\"Home Ownership: {row['home_ownership']}\")\n",
    "    if pd.notna(row.get(\"Amount Requested\")):\n",
    "        text_parts.append(f\"Amount Requested: ${row['Amount Requested']}\")\n",
    "    if pd.notna(row.get(\"annual_inc\")):\n",
    "        text_parts.append(f\"Annual Income: ${row['annual_inc']}\")\n",
    "    if pd.notna(row.get(\"Debt-To-Income Ratio\")):\n",
    "        text_parts.append(f\"DTI Ratio: {row['Debt-To-Income Ratio']}\")\n",
    "    if pd.notna(row.get(\"int_rate\")):\n",
    "        text_parts.append(f\"Interest Rate: {row['int_rate']}\")\n",
    "    if pd.notna(row.get(\"term\")):\n",
    "        text_parts.append(f\"Term: {row['term']}\")\n",
    "    if pd.notna(row.get(\"loan_status\")):\n",
    "        text_parts.append(f\"Status: {row['loan_status']}\")\n",
    "\n",
    "    return \" | \".join(text_parts)\n",
    "\n",
    "def precompute_embeddings(csv_file: str, output_dir: str, batch_size: int = 5000, start_batch: int = 1):\n",
    "    \"\"\"\n",
    "    Precompute embeddings in batches and save to disk.\n",
    "    Each batch is stored as two files:\n",
    "      - embeddings_batch_<n>.npy\n",
    "      - payload_batch_<n>.parquet\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"üöÄ Loading data...\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    numeric_columns = [\"Amount Requested\", \"annual_inc\", \"Debt-To-Income Ratio\", \"int_rate\"]\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    print(f\"üìä Loaded {len(df)} rows\")\n",
    "\n",
    "    print(\"ü§ñ Loading embedding model...\")\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    print(\"ü§ñ Generating text...\")\n",
    "    df[\"loan_text\"] = df.apply(create_loan_text, axis=1)\n",
    "\n",
    "    total_batches = (len(df) - 1) // batch_size + 1\n",
    "    for batch_num in range(start_batch, total_batches + 1):\n",
    "        i = (batch_num - 1) * batch_size\n",
    "        batch_df = df.iloc[i:i+batch_size].copy()\n",
    "\n",
    "        print(f\"üîÑ Encoding batch {batch_num}/{total_batches} ({len(batch_df)} rows)...\")\n",
    "        embeddings = model.encode(batch_df[\"loan_text\"].tolist(),\n",
    "                                  batch_size=512,\n",
    "                                  show_progress_bar=False)\n",
    "\n",
    "        # Save embeddings\n",
    "        np.save(os.path.join(output_dir, f\"embeddings_batch_{batch_num}.npy\"), embeddings)\n",
    "\n",
    "        # Save payloads (metadata)\n",
    "        batch_df.to_parquet(os.path.join(output_dir, f\"payload_batch_{batch_num}.parquet\"))\n",
    "\n",
    "        print(f\"‚úÖ Saved batch {batch_num}\")\n",
    "\n",
    "    print(\"üéâ All embeddings saved!\")\n",
    "\n",
    "\n",
    "def _json_safe_value(v):\n",
    "    # Convert numpy scalars to native Python and sanitize NaN/Inf/NaT\n",
    "    if isinstance(v, (np.generic,)):\n",
    "        v = v.item()\n",
    "    if v is pd.NaT:\n",
    "        return None\n",
    "    if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):\n",
    "        return None\n",
    "    return v\n",
    "\n",
    "def _df_to_json_safe_payloads(df: pd.DataFrame) -> List[dict]:\n",
    "    # Replace Inf/-Inf -> NaN, then NaN/NaT -> None so JSON is valid\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "\n",
    "    safe_records = []\n",
    "    for rec in records:\n",
    "        safe = {}\n",
    "        for k, v in rec.items():\n",
    "            safe[k] = _json_safe_value(v)\n",
    "        safe_records.append(safe)\n",
    "    # quick sanity check on one record\n",
    "    if safe_records:\n",
    "        json.dumps(safe_records[0])\n",
    "    return safe_records\n",
    "\n",
    "def upload_precomputed_batches(client, output_dir: str, collection_name: str = \"lending_club_loans\", recreate_collection: bool = True, vector_size: int = 384, start_batch: int = 1, end_batch: int = None, id_start: int = 0, upload_batch_size: int = 1000, parallel_workers: int = 2, batch_id_block: int = 1_000_000):\n",
    "    \"\"\"\n",
    "    Uploads batches saved as:\n",
    "      - embeddings_batch_<n>.npy\n",
    "      - payload_batch_<n>.parquet\n",
    "    Uses upload_collection per batch (streamed internally).\n",
    "    Cleans payloads to avoid 'cannot convert to json number' errors.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    # Find available batch numbers\n",
    "    npy_pattern = re.compile(r\"embeddings_batch_(\\d+)\\.npy$\")\n",
    "    batch_nums = []\n",
    "    for fname in os.listdir(output_dir):\n",
    "        m = npy_pattern.match(fname)\n",
    "        if m:\n",
    "            batch_nums.append(int(m.group(1)))\n",
    "    batch_nums = sorted(n for n in batch_nums if n >= start_batch and (end_batch is None or n <= end_batch))\n",
    "\n",
    "    if not batch_nums:\n",
    "        print(\"‚ùó No batches found to upload.\")\n",
    "        return\n",
    "\n",
    "    if recreate_collection:\n",
    "        client.recreate_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "        )\n",
    "        print(f\"‚ú® (Re)created collection '{collection_name}'\")\n",
    "    else:\n",
    "        # Ensure collection exists; create if not\n",
    "        try:\n",
    "            info = client.get_collection(collection_name)\n",
    "            print(f\"‚ÑπÔ∏è  Collection '{collection_name}' exists with {info.points_count} points.\")\n",
    "\n",
    "\n",
    "        except Exception:\n",
    "            client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "            )\n",
    "            print(f\"‚ú® Created collection '{collection_name}'\")\n",
    "\n",
    "    running_id = id_start\n",
    "    total_uploaded = (start_batch-1)*5000\n",
    "\n",
    "    for b in batch_nums:\n",
    "        emb_path = os.path.join(output_dir, f\"embeddings_batch_{b}.npy\")\n",
    "        payload_path = os.path.join(output_dir, f\"payload_batch_{b}.parquet\")\n",
    "\n",
    "        if not os.path.exists(payload_path):\n",
    "            print(f\"‚ö†Ô∏è  Missing payload file for batch {b}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üì¶ Loading batch {b}...\")\n",
    "        embeddings = np.load(emb_path)\n",
    "        payload_df = pd.read_parquet(payload_path)\n",
    "\n",
    "        # Ensure payloads are JSON-safe (None instead of NaN/Inf; native types)\n",
    "        payloads = _df_to_json_safe_payloads(payload_df)\n",
    "\n",
    "        # Sanity check: vectors shape should match payload count\n",
    "        if len(payloads) != embeddings.shape[0]:\n",
    "            raise ValueError(f\"Batch {b}: embeddings count {embeddings.shape[0]} != payloads count {len(payloads)}\")\n",
    "\n",
    "        ids = [b * batch_id_block + i for i in range(len(payloads))]\n",
    "\n",
    "        json.dumps(payloads[0])\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        minutes, seconds = divmod(int(elapsed), 60)\n",
    "        formatted_time = f\"{minutes:02d}:{seconds:02d}\"\n",
    "        print(f\"‚¨ÜÔ∏è  Uploading batch {b} ({len(ids)} vectors) ... started at {formatted_time}\")\n",
    "\n",
    "        client.upload_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors=embeddings,      # shape: (N, 384)\n",
    "            payload=payloads,        # list[dict]\n",
    "            ids=ids,                 # list[int]\n",
    "            batch_size=upload_batch_size,\n",
    "            parallel=parallel_workers,\n",
    "        )\n",
    "\n",
    "        running_id += len(ids)\n",
    "        total_uploaded += len(ids)\n",
    "        print(f\"‚úÖ Uploaded batch {b}. Total uploaded so far: {total_uploaded}.\")\n",
    "\n",
    "    # Final count\n",
    "    try:\n",
    "        info = client.get_collection(collection_name)\n",
    "        print(f\"üéâ Done. Collection '{collection_name}' now has {info.points_count} points.\")\n",
    "    except Exception:\n",
    "        print(\"üéâ Done. (Skipped final count fetch.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03acb79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precompute_embeddings('./data/cleaned_combined_lendingclub.csv', \"./data/embeddings\", batch_size=5000, start_batch=446)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb40db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_precomputed_batches(\n",
    "    qdrant_client,\n",
    "    output_dir=\"./data/embeddings\",   # where your .parquet/.npy live\n",
    "    collection_name=\"lending_club_loans\",\n",
    "    recreate_collection=False,           # set False to append\n",
    "    start_batch=339,                      # or resume from any batch number\n",
    "    upload_batch_size=500,             # tune if needed\n",
    "    parallel_workers=4,                 # tune for your network/CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c439ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection has 1688500 points.\n"
     ]
    }
   ],
   "source": [
    "info = qdrant_client.get_collection('lending_club_loans')\n",
    "print(f\"Collection has {info.points_count} points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f7b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overwrite_batches(client, output_dir: str, collection_name: str, source_start_batch: int, source_end_batch: int, target_start_batch: int, upload_batch_size: int = 1000, parallel_workers: int = 2, batch_id_block: int = 1_000_000):\n",
    "    \"\"\"\n",
    "    Overwrite specific batches in Qdrant using locally saved embeddings/payloads.\n",
    "    Instead of creating new IDs, reuse the IDs of the target batches so vectors are overwritten.\n",
    "\n",
    "    Args:\n",
    "        client: Qdrant client\n",
    "        output_dir: directory containing embeddings_batch_<n>.npy and payload_batch_<n>.parquet\n",
    "        collection_name: name of collection in Qdrant\n",
    "        source_start_batch: first local batch number to read from disk\n",
    "        source_end_batch: last local batch number to read from disk (inclusive)\n",
    "        target_start_batch: first batch in Qdrant to overwrite (same size window as source)\n",
    "        upload_batch_size: chunk size for upload\n",
    "        parallel_workers: number of workers for parallel upload\n",
    "        batch_id_block: reserved ID block size per batch (must match original upload)\n",
    "    \"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    source_batches = range(source_start_batch, source_end_batch + 1)\n",
    "\n",
    "    for offset, src_batch in enumerate(source_batches):\n",
    "        tgt_batch = target_start_batch + offset\n",
    "\n",
    "        emb_path = os.path.join(output_dir, f\"embeddings_batch_{src_batch}.npy\")\n",
    "        payload_path = os.path.join(output_dir, f\"payload_batch_{src_batch}.parquet\")\n",
    "\n",
    "        if not (os.path.exists(emb_path) and os.path.exists(payload_path)):\n",
    "            print(f\"‚ö†Ô∏è Missing files for batch {src_batch}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        embeddings = np.load(emb_path)\n",
    "        payload_df = pd.read_parquet(payload_path)\n",
    "\n",
    "        payloads = _df_to_json_safe_payloads(payload_df)\n",
    "\n",
    "        if len(payloads) != embeddings.shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"Batch {src_batch}: embeddings count {embeddings.shape[0]} != payloads count {len(payloads)}\"\n",
    "            )\n",
    "\n",
    "        # üîë Important: reuse IDs from the target batch we want to overwrite\n",
    "        ids = [tgt_batch * batch_id_block + i for i in range(len(payloads))]\n",
    "\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        minutes, seconds = divmod(int(elapsed), 60)\n",
    "        formatted_time = f\"{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "        print(\n",
    "            f\"‚¨ÜÔ∏è Overwriting target batch {tgt_batch} \"\n",
    "            f\"with local batch {src_batch} ({len(ids)} vectors) ... started at {formatted_time}\"\n",
    "        )\n",
    "\n",
    "        client.upload_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors=embeddings,\n",
    "            payload=payloads,\n",
    "            ids=ids,\n",
    "            batch_size=upload_batch_size,\n",
    "            parallel=parallel_workers,\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ Overwrote target batch {tgt_batch} with local batch {src_batch}.\")\n",
    "\n",
    "    # Final collection info\n",
    "    try:\n",
    "        info = client.get_collection(collection_name)\n",
    "        print(f\"üéâ Done. Collection '{collection_name}' now has {info.points_count} points.\")\n",
    "    except Exception:\n",
    "        print(\"üéâ Done. (Skipped final count fetch.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86215db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_batches(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"lending_club_loans\",\n",
    "    output_dir=\"./data/embeddings\",\n",
    "    source_start_batch=590,\n",
    "    source_end_batch=621,\n",
    "    target_start_batch=100,\n",
    "    upload_batch_size=500,       \n",
    "    parallel_workers=4\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
